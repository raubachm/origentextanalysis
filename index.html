<!DOCTYPE html>
<html>
<head>
    <title>Index</title>
    <style>
        body {
            background-color: #f2f2f2;
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            margin-right: 2%;
            margin-left: 2%;
            margin-top: 2%;
            height: 100vh;
            flex-direction: column;
            padding-top: 15%; /* Added padding to the top */
        }

        form {
            background-color: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            display: flex;
            flex-direction: column;
            align-items: center;
            width: 400px; /* Increased width for aesthetics */
            margin-bottom: 20%; /* Added space between form and bottom of the window */
        }

        input[type="text"] {
            margin-bottom: 10px;
            padding: 8px;
            border: 1px solid #ccc;
            border-radius: 5px;
            margin-bottom: 10px;
            width: 90%;
        }

        h1 {
            margin-top: 20px;
            color: #333;
        }

        input[type="submit"] {
            margin-bottom: 20px;
            padding: 10px 20px;
            background-color: #4CAF50;
            color: #fff;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }

        input[type="submit"]:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <h1>Introduction</h1>
    <p>
        This is a web application that utilizes sentence embeddings and cosine similarity to compare similarities between sentences in two different texts. The application is built using the Flask framework in Python. The user is presented with an interface where they can select a pre-trained model for generating sentence embeddings from a text to compare with the embeddings from four of Origen of Alexandria's works: On First Principals, Against Celsus, Commentary on Matthew, and Commentary on John. The text used to create the emebddings is the publicly available translation of Origen's works in <i> Ante-Nicene Fathers, Vol. 4. Edited by Alexander Roberts, James Donaldson, and A. Cleveland Coxe. (Buffalo, NY: Christian Literature Publishing Co., 1885).</i> 
    </p>
    <p>
        The models used to create the emebddings are "all-MiniLM-L6-v2" and "RoBERTa-base." These models are widely used in natural language processing tasks and are known for their effectiveness in capturing semantic information from text. Both "all-MiniLM-L6-v2" and "RoBERTa-base" are trained on large corpora of text data in order to learn rich contextual representations. However, RoBERTa uses dynamic masking strategies during training that increase its robustness compared to BERT, allowing it to achieve better results on various benchmark tests. Alternatively, "all-MiniLM-L6-v2" adopts the concept of minimum length of the input sequence and maximum sequence length, resulting in models with smaller sizes than their counterparts while maintaining adequate capacity for representation learning. Despite sharing common ancestry, i.e., being based on the original BERT design by Google, both models differ in several ways. For example, RoBERTa removes several components present in BERT and adapts additional modifications aimed at improving model quality and generalization capabilities. In contrast, "all-MiniLM-L6-v2" uses less strict hyperparameter settings compared to larger-scale variants like GPT-3, facilitating faster convergence and improved efficiency without sacrificing much accuracy when applied on shorter sequences.
    </p>
    <p>
        To run the application, the user must supply a text. This can be any length, but the longer the text the longer it will take for the embeddings to be generated. The text is split into sentences and sentences of less than 4 words are dropped to increase the effectiveness of the subsequent similarity match. The sentences from the new text are encoded into embeddings using the selected model, and finally the cosine similarity between the embeddings of the original and new sentences is calculated resulting in a similarity matrix,
    </p>
    <p>
        Cosine similarity is a widely used metric for measuring the similarity between two vectors in a high-dimensional space, such as the sentence embeddings generated by the selected model. More presiely, Cosine similarity measures the similarity between two non-zero vectors of an inner product space that measure the "angle" between them, without taking into account the magnitude (or scale). Specifically, if we define our feature vectors, or sentence embeddings, in a d-dimensional space, cosine similarity is calculated via the dot product of said vectors divided by the magnitudes of each vector raised to power d/2. A value close to 1 indicates strong alignment of corresponding angles between vectors, implying higher similarity; whereas a value near zero suggests little correlation. It should be noted, since cosine similarity considers only relative orientation between vectors, scaling factors across instances do not affect the final score, rendering cosine similarity invariant to normalizations. It performs quite favorably against alternative distance metrics such as Euclidean distances because it doesn't assume underlying Gaussian distributions. Instead, it's perfectly suited to assess relationships within discrete spaces or regions, particularly when the distribution is skewed.
    </p>
    <p>
        To provide more detailed insights, the program identifies the top similar sentence pairs by sorting the similarity matrix. It retrieves the indices of the highest similarity scores and selects the corresponding original and new sentences. By selecting the top indices, the program identifies the most similar sentence pairs between the original and new texts. Finally, the program generates a result that includes the original sentence, the corresponding new sentence, and the similarity score for each selected pair. These results allow users to understand which sentences from the new text exhibit the highest similarity to the original text and explore the specific semantic connections between them. Overall, by utilizing cosine similarity, the program enables a quantitative assessment of the similarity between the original and new texts, providing users with valuable insights into the degree of textual similarity.
    </p>

    <p><strong>Model Descriptions:</strong></p>
    <ul>
        <li>
            <strong>all-MiniLM-L6-v2:</strong> This model is a variant of the MiniLM architecture, which is a small-scale version of the popular language model BERT (Bidirectional Encoder Representations from Transformers). MiniLM-L6 refers to a model with six Transformer layers. It has been fine-tuned on large-scale datasets and produces high-quality sentence embeddings. It strikes a balance between computational efficiency and embedding performance.
        </li>
        <br>
        <li>
            <strong>RoBERTa-base:</strong> This model is based on the RoBERTa architecture, which is a variant of BERT optimized and trained on a large corpus of unlabeled text data. RoBERTa-base refers to a model with 12 Transformer layers. It exhibits strong performance across various natural language understanding tasks and benefits from extensive pre-training on diverse textual data.
        </li>
    </ul>

    <h1>Enter the path for the text file to be analyzed:</h1>
    <form method="POST" action="/">
        <input type="text" name="new_text" placeholder="Text file path">
        <label for="model">Select Model:</label>
        <select id="model" name="model">
            <option value="all-MiniLM-L6-v2">all-MiniLM-L6-v2</option>
            <option value="RoBERTa-base">RoBERTa-base</option>
        </select>
        <br><br>
        <input type="submit" value="Submit">
    </form>
</body>
</html>


